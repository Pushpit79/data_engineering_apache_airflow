Stage1 

Task1 :-->
Took the json file and converted it into the parquet file.

Task2 :-->
Explanation

1. Read the Parquet File: Use pd.read_parquet to load the data from output.parquet into a pandas DataFrame.
2. Create the Folder: Use os.makedirs to create a directory named genres if it doesn't already exist.
3. Split the Genres: Use str.split(',') to split the genres column into lists, and then use explode to transform each element of the lists into its own row.
4. Clean Genre Names: Use str.strip() to remove any leading or trailing whitespace from the genre names.
5. Get Unique Genres: Retrieve unique genres from the genres column using df['genres'].unique().

6. Filter and Write Genre Data:
Loop Through Genres: Iterate over each unique genre, filter the DataFrame to include only rows belonging to the current genre, and write this subset to a Parquet file named after the genre.


Task 3 :--> automation using a apache-airflow 
1. Creating a virtual environment and installing the necessary dependencies from a requirements.txt file.
2.  Used APache Airflow 2.2.2 .
3. Initialize Airflow :  airflow db init
4. Start the Airflow Web Server: airflow webserver --port 8080
5. airflow webserver --port 8080
6. Create a new Python file for genre_parquet_pipeline.py DAG . 
